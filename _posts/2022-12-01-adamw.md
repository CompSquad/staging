---
layout: distill
title: Decay No More.
description: Your blog post's abstract.
  This is an example of a distill-style blog post and the main elements it supports.
date: 2023-02-10
htmlwidgets: true

Anonymize when submitting
authors:
  - name: Anonymous


# must be the exact same name as your blogpost
bibliography: 2022-12-01-adamw.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
toc:
  - name: Equations
  - name: Images and Figures
    subsections:
    - name: Interactive Figures
  - name: Citations
  - name: Footnotes
  - name: Code Blocks
  - name: Layouts
  - name: Other Typography?

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## Introduction


### Notation

We denote by $$\alpha > 0$$ the initial learning rate. We use $$\eta_t > 0$$ for a learning rate schedule multiplier. By this, the effective learning rate in iteration $$t$$ is $$\alpha \eta_t$$. We use $$\lambda > 0$$ for the weight decay parameter. 

## Adam 

`Adam` uses an exponentially weighted average (EMA) of stochastic gradients, typically denoted by $$m_t$$, and of the elementwise squared gradients, denoted by $$v_t$$. 

We denote with $$\hat m_t$$ and $$\hat v_t$$ the EMA estimates with bias correction (see <d-cite key="Kingma2015"></d-cite>), this means

$$
\hat m_t = \frac{m_t}{1-\beta_1^t}, \quad \hat v_t = \frac{v_t}{1-\beta_2^t}
$$

where $$\beta_1, \beta_2 \in [0,1)$$. The update formula of `Adam` is given by

$$
w_t = w_{t-1} - \eta_t \alpha \frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}.
$$

Now how would `Adam` handle regularization? The first approach to this was to simply add the gradient of the regularization term $$\frac{\lambda}{2}\|w\|^2$$ on top of the gradient of the loss and then execute `Adam` as indicated above. This is usually referred to as `AdamL2`. 

## AdamW

For training with $$\ell_2$$-regularization, `AdamW` has been proposed in <d-cite key="Loschilov2019"></d-cite> as an alternative to `AdamL2`. In the original paper, the update formula is given as 

$$
w_t = (1-\eta_t \lambda)w_{t-1} - \eta_t \alpha \frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}.
$$


To the best of our knowledge, no theoretical guarantees have yet been provided for `AdamW`. Despite this, the method had considerable practical success: for example, `AdamW` is implemented in the machine learning libraries Tensorflow <d-cite key="Abadi2015"></d-cite> and Pytorch <d-cite key="Paszke2019"></d-cite>. Another example is the `fairseq` library, developped by Facebook Research, which implements many SeqToSeq models. In their codebase, when `Adam` is used with weight decay, it actually uses `AdamW` by default (see [here](https://github.com/facebookresearch/fairseq/blob/main/fairseq/optim/adam.py)). 

We summarize the empirical findings of <d-cite key="Loschilov2019"></d-cite> as follows:

* `AdamW` improves generalization performance compared to `AdamL2` for image classification tasks. In the paper, the authors use a ResNet for the CIFAR10 dataset. They apply ShakeShake-regularization on top of $$\ell_2$$-regularization.

* Another advantage of `AdamW` is stated in the abstract of <d-cite key="Loschilov2019"></d-cite>:
<blockquote>
    We provide empirical evidence that our proposed modification decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam [...].
</blockquote>

What the authors mean by *decoupling* is that if one plots the test accuracy as a heatmap of learning rate and weight decay, the areas with high accuracy are more rectangular and therefore better separable. This is neatly illustrated in the plot below which is taken from Figure 2 in <d-cite key="Loschilov2019"></d-cite>.

{% include figure.html path="assets/img/2022-12-01-adamw/hutter_fig2.png" class="img-fluid" %}

When revisiting the literature on `AdamW` we made an interesting practical observation: the [Pytorch implementation](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) of `AdamW` is actually slightly different to the algorithm proposed in the paper. In Pytorch, the following is implemented:

$$
w_t = (1-\eta_t \alpha \lambda)w_{t-1} - \eta_t \alpha \frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}.
$$

The difference is that the decay factor in the code is $$1-\eta_t \alpha \lambda$$ instead of $$1-\eta_t \lambda$$ in the paper. Clearly, this is equivalent as we can simply reparametrize the weight decay factor $$\lambda$$ to make up for this. However, as the default learning rate $$\alpha=0.001$$ is rather small, this means that practicioners might need to choose rather high values of $$\lambda$$ in order to get sufficiently strong decay. Moreover, this leaves a certain ambiguity when tuned values for $$\lambda$$ are reported in the literature. 


## ProxAdam

In a recent article, <d-cite key="Zhuang2022"></d-cite> revisits the `AdamW` method and tries to explain its practical success. One of their central arguments is that `AdamW` is approximately equal to `Adam` with a proximal update for $$\ell_2$$-regularization. In order to understand this, we first need to introduce the proximal operator, a central concept of convex analysis. 


### A brief introduction to the proximal operator

Proximal algorithms have been studied for decades in the context of (non-differentiable) optimization, way before machine learning was a hot topic. The groundwork of this field has been laid by R. Tyrrell Rockafellar from the 1970's onwards <d-cite key="Rockafellar1976,Rockafellar1998"></d-cite>.
If $$f: \mathbb{R}^n \to \mathbb{R}$$ is convex then the proximal operator is defined as 

$$
\mathrm{prox}_f(x) := \mathrm{argmin}_{z \in \mathbb{R}^n} f(z) + \frac12 \|z-x\|^2.
$$

For many classical regularization functions (e.g. the $$\ell_1$$-norm), the proximal operator can be computed in closed form. This makes it a key ingredient of optimization algorithms for regularized problems. If $$f$$ is differentiable and $$\varphi$$ is convex, assume that we want to solve 

$$
\min_{w \in \mathbb{R}^n} f(w) + \varphi(w).
$$

The classical proximal gradient method in this setting has the update formula

$$
w_{t} = \mathrm{prox}_{\alpha \varphi}\big(w_{t-1}- \alpha \nabla f(w_{t-1})\big),
$$

where $$\alpha>0$$ is a step size (*aka* learning rate).


### Weight decay as a proximal operator

For $$\ell_2$$-regularization, i.e. $$\varphi(w) = \frac{\lambda}{2}\|w\|^2$$, it is easy to show that the proximal operator at $$w$$ is given by $$\frac{1}{1+\lambda}w = (1-\frac{\lambda}{1+\lambda})w$$. Based on this, the authors of <d-cite key="Zhuang2022"></d-cite> propose a proximal version of `Adam`, called `ProxAdam`. It is given by 

$$
w_t = \big(1- \frac{\lambda\eta_t}{1+\lambda\eta_t} \big)w_{t-1} - \frac{\eta_t \alpha}{1+\lambda\eta_t} \frac{\hat m_t}{\epsilon + \hat v_t}
$$




Knowing this, we can now understand why `AdamW` is approximately a proximal version of `Adam`: Taylor-approximation yields that $$$$.

Let us apply this to the coefficients in front of $$w_{t-1}$$ and $$\frac{\hat m_t}{\epsilon + \hat v_t}$$. Then, we get
the authors of <d-cite key="Zhuang2022"></d-cite> argue that doing a first-order Taylor approximation of the coefficients  yields 

$$
w_t = (1-\eta_t \lambda)w_{t-1} - \eta_t \alpha \frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}.
$$

which is exactly the update of `AdamW`. The argument we just made is exactly what is presented in <d-cite key="Zhuang2022"></d-cite>. Hence `AdamW` $$\approx$$ `ProxAdam`.
