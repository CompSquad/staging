---
layout: distill
title: Decay No More.
description: Your blog post's abstract.
  This is an example of a distill-style blog post and the main elements it supports.
date: 2022-12-01
htmlwidgets: true

authors:
  - name: Anonymous


# must be the exact same name as your blogpost
bibliography: 2022-12-01-adamw.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
toc:
  - name: Equations
  - name: Images and Figures
    subsections:
    - name: Interactive Figures
  - name: Citations
  - name: Footnotes
  - name: Code Blocks
  - name: Layouts
  - name: Other Typography?

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---


## Introduction

Weight decay is a regularization technique in machine learning which aims to reduce the norm of the weights. In general, we can see weight decay as 
It dates back at least to 


The exact mechanism of weight decay is still puzzling the machine learning community:

{% twitter https://twitter.com/rasbt/status/1614327550058328064 %}


{% twitter https://twitter.com/deepcohen/status/1617274166570528769 %}

The article mentioned in the second tweet discusses the effect of weight decay for generalization, in particular the interplay with Batch Normalization `(BN)`. 
Batch Normalization describes a module of a network that normalizes the output of the previous layer to have zero mean and variance of one (or a variant of this with learnable mean and variance). We will not go into the details here but refer to [this blog post](https://iclr-blog-track.github.io/2022/03/25/unnormalized-resnets/) <d-cite key="pieterjan2022normalizationisdead"></d-cite> for the interested reader. 

We want to summarize two findings of the paper:

* On the one hand, weight decay has (in theory) no effect on layers with `(BN)`. This is simply due to the fact that `(BN)` makes the output invariant to a rescaling of the weights. 

<blockquote>
Weight decay is widely used in networks with Batch Normalization (Ioffe & Szegedy,
2015). In principle, weight decay regularization should have no effect in this case, since one
can scale the weights by a small factor without changing the network’s predictions. Hence, it
does not meaningfully constrain the network’s capacity. 

—Zhang et al., 2019
</blockquote>

* However, it is also shown in the paper that weight decay on layers with `(BN)` *can* improve accuracy. The authors argue that this is due to an effectively larger learning rate.

This blog post will summarize the development of weight decay specifically for <span style="font-family:monospace">Adam</span>.
We try to shed some light on the following questions:

1. What is the difference between <span style="font-family:monospace">Adam</span> and its weight decay version <span style="font-family:monospace">AdamW</span>? Does the existing literature give a clear answer to the question when  <span style="font-family:monospace">AdamW</span> performs better?
2. How can we motivate the weight decay mechanism of <span style="font-family:monospace">AdamW</span> from an optimization perspective? 
3. The last section will be somewhat explorational: could we come up with different formulas for a weight decay version of <span style="font-family:monospace">Adam</span>?


### Notation

We denote by $$\alpha > 0$$ the initial learning rate. We use $$\eta_t > 0$$ for a learning rate schedule multiplier. By this, the effective learning rate in iteration $$t$$ is $$\alpha \eta_t$$. We use $$\lambda > 0$$ for the weight decay parameter. 

## Adam 

<span style="font-family:monospace">Adam</span> uses an exponentially weighted average (EMA) of stochastic gradients, typically denoted by $$m_t$$, and of the elementwise squared gradients, denoted by $$v_t$$. 

We denote with $$\hat m_t$$ and $$\hat v_t$$ the EMA estimates with bias correction (see <d-cite key="Kingma2015"></d-cite>), this means

$$
\hat m_t = \frac{m_t}{1-\beta_1^t}, \quad \hat v_t = \frac{v_t}{1-\beta_2^t}
$$

where $$\beta_1, \beta_2 \in [0,1)$$. The update formula of <span style="font-family:monospace">Adam</span> is given by

$$
w_t = w_{t-1} - \eta_t \alpha \frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}.
$$

How would <span style="font-family:monospace">Adam</span> handle regularization? The first approach to this was to simply add the regularization term $$\frac{\lambda}{2}\|w\|^2$$ on top of the loss, do backpropagation and then compute on <span style="font-family:monospace">Adam</span> step as outlined above. This is usually referred to as <span style="font-family:monospace">AdamL2</span>. However, research showed that this can be suboptimal and one major contribution to alleviate this was the development of <span style="font-family:monospace">AdamW</span>.

## AdamW

For training with $$\ell_2$$-regularization, Loshchilov and Hutter proposed <span style="font-family:monospace">AdamW</span> in 2019 <d-cite key="Loshchilov2019"></d-cite> as an alternative to <span style="font-family:monospace">AdamL2</span>. In the paper, the update formula is given as 

$$
\tag{AdamW}
w_t = (1-\eta_t \lambda)w_{t-1} - \eta_t \alpha \frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}.
$$


To the best of our knowledge, no theoretical guarantees have yet been provided for <span style="font-family:monospace">AdamW</span>. Despite this, the method has enjoyed considerable practical success: for instance, <span style="font-family:monospace">AdamW</span> is implemented in the machine learning libraries Tensorflow <d-cite key="Abadi2015"></d-cite> and Pytorch <d-cite key="Paszke2019"></d-cite>. Another example is the `fairseq` library, developped by Facebook Research, which implements many SeqToSeq models. In their codebase, when <span style="font-family:monospace">Adam</span> is used with weight decay, <span style="font-family:monospace">AdamW</span> is used by default (see [here](https://github.com/facebookresearch/fairseq/blob/main/fairseq/optim/adam.py)). 

We summarize the empirical findings of <d-cite key="Loshchilov2019"></d-cite> as follows:

* <span style="font-family:monospace">AdamW</span> improves generalization performance compared to <span style="font-family:monospace">AdamL2</span> for image classification tasks. In the paper, the authors use a ResNet-type model (more details later) for the CIFAR10 and Imagenet32 dataset.

* Another advantage of <span style="font-family:monospace">AdamW</span> is stated in the abstract of <d-cite key="Loshchilov2019"></d-cite>:
<blockquote>
    We provide empirical evidence that our proposed modification decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam [...].
</blockquote>

What the authors mean by *decoupling* is that if one plots the test accuracy as a heatmap of learning rate and weight decay, the areas with high accuracy are more rectangular and therefore better separable. We illustrate this conceptually in the plot below which is inspired by Figure 2 in <d-cite key="Loshchilov2019"></d-cite>. The advatnage of a decoupled method is that if one of the two hyperparameters is changed, the optimal value for the other one might still be identical and does not te be retuned.

<div class="row mt-3">
{% include figure.html path="assets/img/2022-12-01-adamw/heatmap.png" class="img-fluid" %}
</div>
<div class="caption">
    Fig. 1: Heatmap of the test accuracy (bright = good accuracy) depending on learning rate and weight decay parameter choice.
</div>

When revisiting the literature on <span style="font-family:monospace">AdamW</span> we made an interesting practical observation: the [Pytorch implementation](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) of <span style="font-family:monospace">AdamW</span> is actually slightly different to the algorithm proposed in the paper. In Pytorch, the following is implemented:

$$
w_t = (1-\eta_t \alpha \lambda)w_{t-1} - \eta_t \alpha \frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}.
$$

The difference is that the decay factor in the code is $$1-\eta_t \alpha \lambda$$ instead of $$1-\eta_t \lambda$$ in the paper. Clearly, this is equivalent as we can simply reparametrize the weight decay factor $$\lambda$$ to make up for this. However, as the default learning rate $$\alpha=0.001$$ is rather small, this means that practicioners might need to choose rather high values of $$\lambda$$ in order to get sufficiently strong decay. Moreover, this leaves a certain ambiguity when tuned values for $$\lambda$$ are reported in the literature. 

## Follow-up work

In a recent article, Zhuang et al. revisit the <span style="font-family:monospace">AdamW</span> method and try to explain its practical success <d-cite key="Zhuang2022"></d-cite>. One of their central arguments is that <span style="font-family:monospace">AdamW</span> is approximately equal to <span style="font-family:monospace">Adam</span> with a proximal update for $$\ell_2$$-regularization. 

Before explaining this in detail, we first want to summarize the empirical findings of <d-cite key="Zhuang2022"></d-cite>:

* When `(BN)` is *deactivated*, <span style="font-family:monospace">AdamW</span> achieves better generalization compared to <span style="font-family:monospace">AdamL2</span> for image classification with a standard ResNet architecture <d-cite key="He2016"></d-cite>.
* When `(BN)` is *activated*, the test accuracy of <span style="font-family:monospace">AdamW</span> and <span style="font-family:monospace">AdamL2</span> are on par. Moreover, the best accuracy is achieved for no weight decay, i.e. $$\lambda=0$$. 

The second result is somewhat stunning as it seems to contradict the results in <d-cite key="Loshchilov2019"></d-cite>, which had shown that <span style="font-family:monospace">AdamW</span> generalizes better than <span style="font-family:monospace">AdamL2</span>.<d-footnote>It seems like the AdamW-paper also used (BN) in their experiments, see https://github.com/loshchil/AdamW-and-SGDW.</d-footnote>

Comparing the details of the experimental setups, we presume the following explanations for this:

* The model that is trained in <d-cite key="Loshchilov2019"></d-cite> is slightly different as it uses a Shake-Shake-Image ResNet.

* From Figure 4 in <d-cite key="Loshchilov2019"></d-cite>, one can observe that the improvement in accuracy for the CIFAR-10 dataset becomes noticeable very late in the training (see also Section 4.3 in <d-cite key="Loshchilov2019"></d-cite>). Thus, depending on the number of epochs after which training is stopped, one can reach different conclusions.

## ProxAdam

The article <d-cite key="Zhuang2022"></d-cite> not only compares <span style="font-family:monospace">AdamL2</span> to <span style="font-family:monospace">AdamW</span> experimentaly, but also provides a mathematical motivation for weight decay. In order to understand this, we first need to introduce the proximal operator, a central concept of convex analysis. 


### A brief introduction to the proximal operator

Proximal algorithms have been studied for decades in the context of (non-differentiable) optimization, way before machine learning was a thing. The groundwork of this field has been laid by R. Tyrrell Rockafellar from the 1970's onwards <d-cite key="Rockafellar1976,Rockafellar1998"></d-cite>.
If $$f: \mathbb{R}^n \to \mathbb{R}$$ is convex then the proximal operator is defined as 

$$
\mathrm{prox}_f(x) := \mathrm{argmin}_{z \in \mathbb{R}^n} f(z) + \frac12 \|z-x\|^2.
$$

For many classical regularization functions (e.g. the $$\ell_1$$-norm), the proximal operator can be computed in closed form. This makes it a key ingredient of optimization algorithms for regularized problems. If $$f$$ is differentiable and $$\varphi$$ is convex, assume that we want to solve 

$$
\min_{w \in \mathbb{R}^n} f(w) + \varphi(w).
$$

The classical proximal gradient method in this setting has the update formula

$$
w_{t} = \mathrm{prox}_{\alpha \varphi}\big(w_{t-1}- \alpha \nabla f(w_{t-1})\big),
$$

where $$\alpha>0$$ is a step size (*aka* learning rate). An equivalently way of writing this (which will become useful later on) is

$$
w_{t} =  \mathrm{argmin}_y \langle y-w_{t-1}, \nabla f(w_{t-1})\rangle + \varphi(y) + \frac{1}{2\alpha}\|y-w_{t-1}\|^2.
$$

### Weight decay as a proximal operator

For $$\ell_2$$-regularization $$\varphi(w) = \frac{\lambda}{2}\|w\|^2$$, the proximal operator at $$w$$ is given by $$\frac{1}{1+\lambda}w = (1-\frac{\lambda}{1+\lambda})w$$. Based on this, the authors of <d-cite key="Zhuang2022"></d-cite> propose a proximal version of <span style="font-family:monospace">Adam</span> called <span style="font-family:monospace">ProxAdam</span>. It is given by 

$$
\tag{ProxAdam}
w_t = \big(1- \frac{\lambda\eta_t}{1+\lambda\eta_t} \big)w_{t-1} - \frac{\eta_t \alpha}{1+\lambda\eta_t} \frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}.
$$

Knowing this, we can now understand why <span style="font-family:monospace">AdamW</span> is approximately a proximal version of <span style="font-family:monospace">Adam</span>: Taylor-approximation yields that $$\frac{ax}{1+bx}\approx ax$$ for small $$x$$. Let us apply this to the coefficients in front of $$w_{t-1}$$ and $$\frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}$$. We get the formula

$$
w_t = (1-\eta_t \lambda)w_{t-1} - \eta_t \alpha \frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}
$$

which is equal to <span style="font-family:monospace">AdamW</span>. The argument we just presented is exactly how <d-cite key="Zhuang2022"></d-cite> concludes that <span style="font-family:monospace">AdamW</span> $$\approx$$ <span style="font-family:monospace">ProxAdam</span>.

### Changing the norm

There is one more way of interpreting proximal methods. Let us begin with a simple example: Define the diagonal matrix $$D_t := \mathrm{Diag}(\epsilon + \sqrt{\hat v_t})$$. Then, the <span style="font-family:monospace">Adam</span> update can be equivalently written as

$$
w_t = \mathrm{argmin}_y \langle y-w_{t-1}, \hat m_t \rangle + \frac{1}{2\eta_t\alpha}\|y-w_{t-1}\|_{D_t}^2.
$$

In other words, <span style="font-family:monospace">Adam</span> takes a proximal step of a linear function, but with the adaptive norm $$D_t$$. This change in norm is (up to the EMA) what makes <span style="font-family:monospace">Adam</span> different from <span style="font-family:monospace">SGD</span>.

We can derive the update formula of <span style="font-family:monospace">ProxAdam</span> analogously:

$$
\tag{P1}
w_t = \mathrm{argmin}_y \langle y-w_{t-1}, \hat m_t \rangle + \frac{\lambda}{2\alpha}\|y\|_{D_t}^2 + \frac{1}{2\alpha \eta_t}\|y-w_{t-1}\|_{D_t}^2.
$$

In fact, the first-order optimality conditions of (P1) are

$$
0 = \hat m_t + \frac{\lambda}{\alpha} D_t w_t + \frac{1}{\alpha \eta_t}D_t (w_t-w_{t-1}).
$$

Solving for $$w_t$$ (and doing simple algebra) gives

$$
w_t = (1+\lambda \eta_t)^{-1}\big[w_{t-1} - \alpha \eta_t D_t^{-1} \hat m_t\big],
$$

which is equal to <span style="font-family:monospace">ProxAdam</span>. 
What is slightly surprising here is the term $$\|y\|_{D_t}^2$$ in (P1) as this implies that we use a different norm also for the regularization term in <span style="font-family:monospace">ProxAdam</span>. This leads us to the final part.

## What else?

There is one more tweak in interpreting <span style="font-family:monospace">AdamW</span> as a proximal method.

 Now, a natural way to handle $$\ell_2$$-regularization would be to solve instead

$$
w_t = \mathrm{argmin}_y \langle y-w_{t-1}, \hat m_t \rangle + \frac{\lambda}{2}\|y\|^2 + \frac{1}{2\eta_t\alpha}\|y-w_{t-1}\|_{D_t}^2.
$$

The above problem can be solved in closed form using its first-order optimality conditions. With a little bit of algebra, one gets 

$$
w_t = w_{t-1} - \frac{\lambda\eta_t\alpha}{D_t + \lambda \eta_t\alpha} w_{t-1} - \eta_t\alpha \frac{\hat m_t}{D_t + \lambda\eta_t\alpha}.
$$

Let us call this method <span style="font-family:monospace">AdamP</span>. Its first order Taylor approximation is

$$
w_t = \big(\mathrm{Id} - \frac{\lambda\eta_t\alpha}{D_t} \big)w_{t-1} - \eta_t\alpha \frac{\hat m_t}{D_t}.
$$

Comparing this to <span style="font-family:monospace">AdamW</span> (or <span style="font-family:monospace">ProxAdam</span>) we see that the second term is the same, but now the decay also depends on $$D_t$$. 

## Summary

* Weight decay can be seen as a proximal way of handling $$\ell_2$$-regularization. 
* Whether or not weight decay brings advantages when used *together with* `(BN)` seems to depend on several factors of the model and experimental design. However, in all experiments we discussed here <span style="font-family:monospace">AdamW</span> seems to be better or at least on par to <span style="font-family:monospace">AdamL2</span>. This suggests that proximal algorithms (with the special case of weight decay) are indeed favourable. 