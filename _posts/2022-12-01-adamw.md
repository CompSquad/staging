---
layout: distill
title: Decay No More.
description: Your blog post's abstract.
  This is an example of a distill-style blog post and the main elements it supports.
date: 2023-02-10
htmlwidgets: true

Anonymize when submitting
authors:
  - name: Anonymous


# must be the exact same name as your blogpost
bibliography: 2022-12-01-adamw.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
toc:
  - name: Equations
  - name: Images and Figures
    subsections:
    - name: Interactive Figures
  - name: Citations
  - name: Footnotes
  - name: Code Blocks
  - name: Layouts
  - name: Other Typography?

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## Introduction


The effect of weight decay is still not fully understood and discussed extensively in the Machine Learning community. A few examples:

{% twitter https://twitter.com/rasbt/status/1614327550058328064 %}


{% twitter https://twitter.com/deepcohen/status/1617274166570528769 %}

The article mentioned in the second tweet discusses the effect of weight decay for generalization, in particular the interplay with Batch Normalization (`BN`). We want to summarize two findings of the paper:

1) On the one hand, weight decay has (in theory) no effect on layers with `BN`. 

<blockquote>
Weight decay is widely used in networks with Batch Normalization (Ioffe & Szegedy,
2015). In principle, weight decay regularization should have no effect in this case, since one
can scale the weights by a small factor without changing the network’s predictions. Hence, it
does not meaningfully constrain the network’s capacity. 

—Zhang et al., 2019
</blockquote>


2) However, it is also shown in the paper that weight decay on layers with `BN` *can* improve accuracy. The authors argue that this is due to an effectively larger learning rate.

This blog post will summarize the development of weight decay specifically for `Adam`.
We try to shed some light on the following questions:

1) What is the algorithmic difference between `Adam` and its weight decay version `AdamW` and how do they compare in practice?
2) How can we motivate the weight decay (or `AdamW`) mechanism from an optimization perspective?


### Notation

We denote by $$\alpha > 0$$ the initial learning rate. We use $$\eta_t > 0$$ for a learning rate schedule multiplier. By this, the effective learning rate in iteration $$t$$ is $$\alpha \eta_t$$. We use $$\lambda > 0$$ for the weight decay parameter. 

## Adam 

`Adam` uses an exponentially weighted average (EMA) of stochastic gradients, typically denoted by $$m_t$$, and of the elementwise squared gradients, denoted by $$v_t$$. 

We denote with $$\hat m_t$$ and $$\hat v_t$$ the EMA estimates with bias correction (see <d-cite key="Kingma2015"></d-cite>), this means

$$
\hat m_t = \frac{m_t}{1-\beta_1^t}, \quad \hat v_t = \frac{v_t}{1-\beta_2^t}
$$

where $$\beta_1, \beta_2 \in [0,1)$$. The update formula of `Adam` is given by

$$
w_t = w_{t-1} - \eta_t \alpha \frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}.
$$

How would `Adam` handle regularization? The first approach to this was to simply add the gradient of the regularization term $$\frac{\lambda}{2}\|w\|^2$$ on top of the gradient of the loss and then execute `Adam` as outlined above. This is usually referred to as `AdamL2`. However, research showed that this can be suboptimal and one major contribution to alleviate this was the development of `AdamW`.

## AdamW

For training with $$\ell_2$$-regularization, `AdamW` has been proposed in <d-cite key="Loschilov2019"></d-cite> as an alternative to `AdamL2`. In the original paper, the update formula is given as 

$$
w_t = (1-\eta_t \lambda)w_{t-1} - \eta_t \alpha \frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}.
$$


To the best of our knowledge, no theoretical guarantees have yet been provided for `AdamW`. Despite this, the method had considerable practical success: for example, `AdamW` is implemented in the machine learning libraries Tensorflow <d-cite key="Abadi2015"></d-cite> and Pytorch <d-cite key="Paszke2019"></d-cite>. Another example is the `fairseq` library, developped by Facebook Research, which implements many SeqToSeq models. In their codebase, when `Adam` is used with weight decay, it actually uses `AdamW` by default (see [here](https://github.com/facebookresearch/fairseq/blob/main/fairseq/optim/adam.py)). 

We summarize the empirical findings of <d-cite key="Loschilov2019"></d-cite> as follows:

* `AdamW` improves generalization performance compared to `AdamL2` for image classification tasks. In the paper, the authors use a ResNet-type model (more details later) for the CIFAR10 and Imagenet32 dataset.

* Another advantage of `AdamW` is stated in the abstract of <d-cite key="Loschilov2019"></d-cite>:
<blockquote>
    We provide empirical evidence that our proposed modification decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam [...].
</blockquote>

What the authors mean by *decoupling* is that if one plots the test accuracy as a heatmap of learning rate and weight decay, the areas with high accuracy are more rectangular and therefore better separable. This is neatly illustrated in the plot below which is taken from Figure 2 in <d-cite key="Loschilov2019"></d-cite>.

{% include figure.html path="assets/img/2022-12-01-adamw/hutter_fig2.png" class="img-fluid" %}

When revisiting the literature on `AdamW` we made an interesting practical observation: the [Pytorch implementation](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) of `AdamW` is actually slightly different to the algorithm proposed in the paper. In Pytorch, the following is implemented:

$$
w_t = (1-\eta_t \alpha \lambda)w_{t-1} - \eta_t \alpha \frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}.
$$

The difference is that the decay factor in the code is $$1-\eta_t \alpha \lambda$$ instead of $$1-\eta_t \lambda$$ in the paper. Clearly, this is equivalent as we can simply reparametrize the weight decay factor $$\lambda$$ to make up for this. However, as the default learning rate $$\alpha=0.001$$ is rather small, this means that practicioners might need to choose rather high values of $$\lambda$$ in order to get sufficiently strong decay. Moreover, this leaves a certain ambiguity when tuned values for $$\lambda$$ are reported in the literature. 

## Follow-up work

In a recent article, <d-cite key="Zhuang2022"></d-cite> revisits the `AdamW` method and tries to explain its practical success. One of their central arguments is that `AdamW` is approximately equal to `Adam` with a proximal update for $$\ell_2$$-regularization. 

Before we explain this in detail, we again summarize the empirical findings of <d-cite key="Zhuang2022"></d-cite>:

* When `BN` is *deactivated*, `AdamW` achieves better generalization compared to `AdamL2`
* When `BN` is *activated*, the test accuracy of `AdamW` and `AdamL2` are on par. Moreover, the best accuracy is achieved for no weight decay, i.e. $$\lambda=0$$. 

The second result is somewhat stunning as it seems to contradict the results in <d-cite key="Loschilov2019"></d-cite><d-footnote>From the [provided codebase](https://github.com/loshchil/AdamW-and-SGDW) it seems that <d-cite key="Loschilov2019"></d-cite> does use `BN`.</d-footnote>

Comparing the details of the experimental setups, we presume the following explanations for this:

* The model that is trained in <d-cite key="Loschilov2019"></d-cite> is slightly different as it uses a Shake-Shake-Image ResNet.

* From the plot below (Figure 4 in <d-cite key="Loschilov2019"></d-cite>), one can observe that the improvement in accuracy for the CIFAR-10 dataset becomes noticeable very late in the training (see also Section 4.3 in <d-cite key="Loschilov2019"></d-cite>). So depending on the number of epochs after which training is stopped one can reach different conclusions.

{% include figure.html path="assets/img/2022-12-01-adamw/hutter_fig4.png" class="img-fluid" %}

## ProxAdam

The article <d-cite key="Zhuang2022"></d-cite> did not only compare `AdamL2` to `AdamW` in experiments, but it also provides a mathematical motivation for weight decay. In order to understand this, we first need to introduce the proximal operator, a central concept of convex analysis. 


### A brief introduction to the proximal operator

Proximal algorithms have been studied for decades in the context of (non-differentiable) optimization, way before machine learning was a hot topic. The groundwork of this field has been laid by R. Tyrrell Rockafellar from the 1970's onwards <d-cite key="Rockafellar1976,Rockafellar1998"></d-cite>.
If $$f: \mathbb{R}^n \to \mathbb{R}$$ is convex then the proximal operator is defined as 

$$
\mathrm{prox}_f(x) := \mathrm{argmin}_{z \in \mathbb{R}^n} f(z) + \frac12 \|z-x\|^2.
$$

For many classical regularization functions (e.g. the $$\ell_1$$-norm), the proximal operator can be computed in closed form. This makes it a key ingredient of optimization algorithms for regularized problems. If $$f$$ is differentiable and $$\varphi$$ is convex, assume that we want to solve 

$$
\min_{w \in \mathbb{R}^n} f(w) + \varphi(w).
$$

The classical proximal gradient method in this setting has the update formula

$$
w_{t} = \mathrm{prox}_{\alpha \varphi}\big(w_{t-1}- \alpha \nabla f(w_{t-1})\big),
$$

where $$\alpha>0$$ is a step size (*aka* learning rate).


### Weight decay as a proximal operator

For $$\ell_2$$-regularization, i.e. $$\varphi(w) = \frac{\lambda}{2}\|w\|^2$$, it is easy to show that the proximal operator at $$w$$ is given by $$\frac{1}{1+\lambda}w = (1-\frac{\lambda}{1+\lambda})w$$. Based on this, the authors of <d-cite key="Zhuang2022"></d-cite> propose a proximal version of `Adam`, called `ProxAdam`. It is given by 

$$
w_t = \big(1- \frac{\lambda\eta_t}{1+\lambda\eta_t} \big)w_{t-1} - \frac{\eta_t \alpha}{1+\lambda\eta_t} \frac{\hat m_t}{\epsilon + \hat v_t}
$$




Knowing this, we can now understand why `AdamW` is approximately a proximal version of `Adam`. Taylor-approximation yields that $$\frac{ax}{1+bx}\approx ax$$ for small $$x$$.

Let us apply this to the coefficients in front of $$w_{t-1}$$ and $$\frac{\hat m_t}{\epsilon + \hat v_t}$$. Then, we get the formula

$$
w_t = (1-\eta_t \lambda)w_{t-1} - \eta_t \alpha \frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}
$$

which is exactly the update of `AdamW`. The argument we just made is exactly what is presented in <d-cite key="Zhuang2022"></d-cite>. Hence `AdamW` $$\approx$$ `ProxAdam`.

## What else?

There is one more tweak in interpreting `AdamW` as a proximal method.

Define the diagonal matrix $$D_t := \mathrm{Diag}(\epsilon + \sqrt{\hat v_t})$$. Then, the `Adam` update can be equivalently written as

$$
x_t = \mathrm{argmin}_y \langle y-x_{t-1}, \hat m_t \rangle + \frac{1}{2\eta_t\alpha}\|y-x_{t-1}\|_{D_t}^2.
$$

In other words, `Adam` takes a proximal step of a linear function, but with a different norm than. This change in norm is (up to the EMA) what makes `Adam` different from `SGD`. Now, a natural way to handle $$\ell_2$$-regularization would be to solve instead

$$
x_t = \mathrm{argmin}_y \langle y-x_{t-1}, \hat m_t \rangle + \frac{\lambda}{2}\|y\|^2 + \frac{1}{2\eta_t\alpha}\|y-x_{t-1}\|_{D_t}^2.
$$

The above problem can be solved in closed form using first-order optimality conditions. With a little bit of algebra, one gets 

$$
w_t = w_{t-1} - \frac{\lambda\eta_t\alpha}{D_t + \lambda \eta_t\alpha} w_{t-1} - \eta_t\alpha \frac{\hat m_t}{D_t + \lambda\eta_t\alpha}.
$$

Again, the first order Taylor approximation for this is

$$
w_t = \big(\mathrm{Id} - \frac{\lambda\eta_t\alpha}{D_t} \big)w_{t-1} - \eta_t\alpha \frac{\hat m_t}{D_t}.
$$

Comparing this to `AdamW` (or `ProxAdam`) we see that the second term is the same, but now the decay also depends on $$D_t$$. 

## Summary

* Weight decay can be seen as a proximal way of handling $$\ell_2$$-regularization. 
* Whether or not weight decay brings advantages when used *together with* `BN` seems to depend on several factors of the model and experimental design. However, in all experiments we discussed here `AdamW` seems to be better or at least on par to `AdamL2`. This suggests that proximal algorithms (with the special case of weight decay) are indeed favourable. 